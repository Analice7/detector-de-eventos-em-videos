import torch
import asyncio
try:
    asyncio.get_event_loop()
except RuntimeError:
    asyncio.set_event_loop(asyncio.new_event_loop())
    
import streamlit as st
import cv2
import numpy as np
import tempfile
import os
import time
from PIL import Image
import pandas as pd
import matplotlib.pyplot as plt
from pathlib import Path
import sys
import importlib
import importlib.util

# Adicionar o diret√≥rio raiz ao PATH para permitir imports relativos
import os
import sys
from pathlib import Path

# Determinar o caminho raiz do projeto
# Assumindo que este arquivo est√° em src/app/app.py
current_path = Path(os.path.abspath(__file__))
root_dir = current_path.parent.parent.parent  # Subir tr√™s n√≠veis: file -> app -> src -> raiz
sys.path.append(str(root_dir))

# Agora podemos importar m√≥dulos de outras pastas
from src.preprocessing.dataProcesser import download_youtube_video, extrair_frames, pre_processar_frame
from src.preprocessing.keypoints import extract_keypoints_from_frames

# Configura√ß√£o da p√°gina Streamlit
st.set_page_config(
    page_title="Detector de Assaltos em V√≠deos de Seguran√ßa",
    page_icon="üé•",
    layout="wide"
)

# Fun√ß√£o para visualizar frames com keypoints sobrepostos (simula√ß√£o)
def visualize_frame_with_keypoints(frame, keypoints=None):
    img = frame.copy()
    if keypoints is not None:
        # Visualiza√ß√£o de keypoints - este √© um placeholder
        # Ser√° substitu√≠do com a visualiza√ß√£o real dos keypoints quando integrado com o modelo
        height, width = img.shape[:2]
        for person in keypoints:
            for x, y in person:
                x_px, y_px = int(x * width), int(y * height)
                cv2.circle(img, (x_px, y_px), 5, (0, 255, 0), -1)
            
            # Conectar keypoints para formar um esqueleto (isso √© um placeholder)
            # Ser√° substitu√≠do com conex√µes reais dos keypoints do seu modelo
    return img

# Interface Streamlit
st.title("üé• Detector de Assaltos em V√≠deos de C√¢meras de Seguran√ßa")
st.markdown("""
Esta aplica√ß√£o permite baixar, processar e detectar assaltos em v√≠deos de c√¢meras de seguran√ßa.
Insira a URL de um v√≠deo do YouTube ou fa√ßa upload de um v√≠deo para an√°lise.
""")

# Criar abas para organizar a interface
tab1, tab2, tab3, tab4 = st.tabs(["Carregamento de V√≠deo", "Pr√©-processamento", "Extra√ß√£o de Poses", "Classifica√ß√£o"])

# Aba 1: Carregamento de V√≠deo
with tab1:
    st.header("Carregamento de V√≠deo")
    
    # Input de URL do YouTube
    youtube_url = st.text_input("URL do v√≠deo do YouTube", "")
    
    # Upload de v√≠deo
    uploaded_file = st.file_uploader("Ou fa√ßa upload de um v√≠deo", type=["mp4", "avi", "mov"])
    
    # Bot√£o para baixar/carregar o v√≠deo
    if st.button("Carregar V√≠deo"):
        if youtube_url:
            with st.spinner("Baixando v√≠deo do YouTube..."):
                try:
                    # Usar a fun√ß√£o existente do seu c√≥digo
                    video_path, video_title = download_youtube_video(youtube_url)
                    st.session_state.video_path = video_path
                    st.session_state.video_title = video_title
                    st.success(f"V√≠deo baixado com sucesso: {video_title}")
                    st.video(video_path)
                except Exception as e:
                    st.error(f"Erro ao baixar o v√≠deo: {str(e)}")
        
        elif uploaded_file:
            with st.spinner("Salvando v√≠deo enviado..."):
                try:
                    temp_dir = tempfile.mkdtemp()
                    video_path = os.path.join(temp_dir, "uploaded_video.mp4")
                    with open(video_path, "wb") as f:
                        f.write(uploaded_file.read())
                    st.session_state.video_path = video_path
                    st.session_state.video_title = "V√≠deo enviado"
                    st.success("V√≠deo enviado com sucesso!")
                    st.video(video_path)
                except Exception as e:
                    st.error(f"Erro ao salvar o v√≠deo: {str(e)}")
    
    # Exibir informa√ß√µes do v√≠deo se dispon√≠vel
    if 'video_path' in st.session_state and 'video_title' in st.session_state:
        st.subheader(f"V√≠deo Carregado: {st.session_state.video_title}")
        
        # Extrair informa√ß√µes b√°sicas do v√≠deo
        cap = cv2.VideoCapture(st.session_state.video_path)
        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        duration = frame_count / fps if fps > 0 else 0
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        cap.release()
        
        # Exibir informa√ß√µes em colunas
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Total de Frames", f"{frame_count}")
        with col2:
            st.metric("FPS", f"{fps}")
        with col3:
            st.metric("Dura√ß√£o", f"{duration:.2f} segundos")
        
        st.metric("Resolu√ß√£o", f"{width}x{height}")

# Aba 2: Pr√©-processamento
with tab2:
    st.header("Pr√©-processamento de Frames")
    
    if 'video_path' not in st.session_state:
        st.warning("Carregue um v√≠deo primeiro na aba 'Carregamento de V√≠deo'")
    else:
        # Par√¢metros de pr√©-processamento
        st.subheader("Par√¢metros de Extra√ß√£o de Frames")
        frame_interval = st.slider("Intervalo de frames para an√°lise", 1, 30, 5, 
                                  help="Extrair 1 frame a cada N frames do v√≠deo")
        
        # Op√ß√µes de pr√©-processamento
        st.subheader("Op√ß√µes de Pr√©-processamento")
        col1, col2 = st.columns(2)
        with col1:
            equalizar = st.checkbox("Equalizar histograma", value=True)
            remover_ruido = st.checkbox("Remover ru√≠do", value=True)
        with col2:
            tracar_contorno = st.checkbox("Tra√ßar contorno", value=True)
            tamanho = st.selectbox("Tamanho de sa√≠da", [
                "320x240", "640x480", "800x600", "1280x720"
            ], index=1)
            width, height = map(int, tamanho.split("x"))
        
        # Bot√£o para iniciar o pr√©-processamento
        if st.button("Extrair e Pr√©-processar Frames"):
            with st.spinner("Extraindo frames..."):
                try:
                    # Usar a fun√ß√£o existente do seu c√≥digo
                    frames = extrair_frames(st.session_state.video_path, f=frame_interval)
                    st.session_state.frames = frames
                    st.success(f"Extra√ß√£o conclu√≠da! {len(frames)} frames extra√≠dos.")
                    
                    # Pr√©-processar frames
                    frames_processados = []
                    for frame in frames:
                        processed = pre_processar_frame(
                            frame,
                            tamanho=(width, height),
                            equalizar=equalizar,
                            remover_ruido=remover_ruido,
                            tracar_contorno=tracar_contorno
                        )
                        frames_processados.append(processed)
                    
                    st.session_state.frames_processados = frames_processados
                    
                    # Mostrar alguns frames de exemplo
                    st.subheader("Visualiza√ß√£o de Frames")
                    num_frames_to_show = min(5, len(frames))
                    cols = st.columns(num_frames_to_show)
                    
                    for i, col in enumerate(cols):
                        idx = i * len(frames) // num_frames_to_show
                        with col:
                            # Converter frame em escala de cinza para RGB para exibi√ß√£o
                            if len(frames_processados[idx].shape) == 2:  # Se for escala de cinza
                                display_frame = cv2.cvtColor(frames_processados[idx], cv2.COLOR_GRAY2RGB)
                            else:
                                display_frame = frames_processados[idx]
                            
                            st.image(display_frame, caption=f"Frame {idx}", use_column_width=True)
                    
                except Exception as e:
                    st.error(f"Erro no pr√©-processamento: {str(e)}")

# Aba 3: Extra√ß√£o de Poses
with tab3:
    st.header("Extra√ß√£o de Keypoints (Poses)")
    
    if 'frames' not in st.session_state:
        st.warning("Extraia os frames primeiro na aba 'Pr√©-processamento'")
    else:
        st.info("Esta etapa executar√° o modelo YOLO para detec√ß√£o de poses e extrair√° os keypoints de cada frame.")
        
        # Criar caminhos de diret√≥rio relativos ao diret√≥rio raiz do projeto
        default_output_dir = os.path.join(str(root_dir), "data", "processed", "keypoints", "temp")
        
        # Diret√≥rio para salvar os keypoints
        output_dir = st.text_input(
            "Diret√≥rio para salvar os keypoints", 
            value=default_output_dir,
            help="Os keypoints ser√£o salvos neste diret√≥rio como arquivos .npy"
        )
        
        # Bot√£o para iniciar a extra√ß√£o de keypoints
        if st.button("Extrair Keypoints"):
            # Criar diret√≥rio se n√£o existir
            os.makedirs(output_dir, exist_ok=True)
            
            with st.spinner("Extraindo keypoints dos frames..."):
                try:
                    # Usar a fun√ß√£o existente para extrair keypoints
                    extract_keypoints_from_frames(st.session_state.frames, output_dir)
                    st.session_state.keypoints_dir = output_dir
                    st.success(f"Keypoints extra√≠dos com sucesso e salvos em: {output_dir}")
                    
                    # Mostrar alguns frames com keypoints sobrepostos (simula√ß√£o)
                    st.subheader("Visualiza√ß√£o de Keypoints (Simula√ß√£o)")
                    
                    # Carregar os primeiros arquivos .npy para visualiza√ß√£o
                    keypoints_files = sorted(Path(output_dir).glob("*.npy"))
                    if keypoints_files:
                        keypoints_samples = []
                        for i in range(min(3, len(keypoints_files))):
                            sample_keypoints = np.load(keypoints_files[i])
                            keypoints_samples.append(sample_keypoints)
                        
                        # Mostrar frames com keypoints
                        cols = st.columns(len(keypoints_samples))
                        for i, (col, kpts) in enumerate(zip(cols, keypoints_samples)):
                            with col:
                                # Sobrepor keypoints nos frames originais
                                frame_with_keypoints = visualize_frame_with_keypoints(
                                    st.session_state.frames[i], kpts
                                )
                                st.image(frame_with_keypoints, caption=f"Frame {i} com keypoints", use_column_width=True)
                    
                except Exception as e:
                    st.error(f"Erro na extra√ß√£o de keypoints: {str(e)}")

# Aba 4: Classifica√ß√£o
with tab4:
    st.header("Detec√ß√£o de Assaltos")
    
    if 'keypoints_dir' not in st.session_state:
        st.warning("Extraia os keypoints primeiro na aba 'Extra√ß√£o de Poses'")
    else:
        st.info("Esta etapa utilizar√° um modelo de classifica√ß√£o para identificar assaltos nos keypoints extra√≠dos.")
        
        # Sele√ß√£o do modelo
        model_type = st.selectbox(
            "Modelo de Classifica√ß√£o", 
            ["LSTM", "GRU", "BiLSTM", "CNN+LSTM"],
            help="Selecione o tipo de modelo para detec√ß√£o de assaltos"
        )
        
        # Par√¢metros do modelo
        st.subheader("Par√¢metros do Modelo")
        seq_length = st.slider("Comprimento da sequ√™ncia", 5, 50, 20)
        threshold = st.slider("Limiar de confian√ßa", 0.0, 1.0, 0.7, 
                            help="Limiar acima do qual um evento ser√° classificado como assalto")
        
        # Bot√£o para classificar eventos
        if st.button("Detectar Assaltos"):
            with st.spinner("Analisando v√≠deo para detec√ß√£o de assaltos..."):
                # Esta se√ß√£o ser√° integrada com seu modelo real
                st.info("Esta funcionalidade ser√° integrada com seu modelo de classifica√ß√£o.")
                
                # Placeholder para visualiza√ß√£o dos resultados (simula√ß√£o)
                st.subheader("Resultados da Detec√ß√£o (Simula√ß√£o)")
                
                # Simular alguns resultados
                resultados = []
                
                for i in range(len(st.session_state.frames) // seq_length):
                    # Simula√ß√£o de classifica√ß√£o - ser√° substitu√≠da pelo seu modelo real
                    confianca = np.random.random()  # Simular probabilidade de assalto
                    
                    resultados.append({
                        "sequencia": i,
                        "frames": f"{i*seq_length} - {(i+1)*seq_length-1}",
                        "tempo_inicio": f"{(i*seq_length/30):.2f}s",
                        "tempo_fim": f"{((i+1)*seq_length-1)/30:.2f}s",
                        "assalto_detectado": confianca > threshold,
                        "confianca": confianca
                    })
                
                # Mostrar tabela de resultados
                df_resultados = pd.DataFrame(resultados)
                
                # Filtrar apenas sequ√™ncias com assaltos detectados
                df_assaltos = df_resultados[df_resultados["assalto_detectado"] == True]
                
                if len(df_assaltos) > 0:
                    st.success(f"Foram detectados poss√≠veis assaltos em {len(df_assaltos)} segmentos do v√≠deo!")
                    st.dataframe(df_assaltos[["sequencia", "tempo_inicio", "tempo_fim", "confianca"]])
                else:
                    st.info("Nenhum assalto foi detectado neste v√≠deo.")
                
                # Mostrar gr√°fico de confian√ßa ao longo do tempo
                st.subheader("N√≠vel de Confian√ßa ao Longo do V√≠deo")
                
                fig, ax = plt.subplots(figsize=(10, 6))
                ax.plot(df_resultados.index, df_resultados["confianca"], marker='o', linestyle='-')
                ax.axhline(y=threshold, color='r', linestyle='--', label=f'Limiar ({threshold})')
                ax.set_xlabel('Segmento de V√≠deo')
                ax.set_ylabel('Confian√ßa de Detec√ß√£o')
                ax.set_title('Probabilidade de Assalto ao Longo do Tempo')
                ax.legend()
                ax.grid(True, linestyle='--', alpha=0.7)
                st.pyplot(fig)
                
                # Op√ß√£o para baixar o relat√≥rio
                st.subheader("Relat√≥rio")
                csv = df_resultados.to_csv(index=False)
                st.download_button(
                    label="Baixar relat√≥rio em CSV",
                    data=csv,
                    file_name=f"relatorio_assaltos_{st.session_state.video_title.replace(' ', '_')}.csv",
                    mime='text/csv',
                )

# Adicionar informa√ß√µes sobre o projeto na barra lateral
st.sidebar.header("Par√¢metros do Projeto")

st.sidebar.subheader("Detec√ß√£o de Poses")
st.sidebar.selectbox("Modelo de Detec√ß√£o", ["YOLOv8-Pose (nano)", "YOLOv8-Pose (small)", "MediaPipe"])

# Sobre o projeto
st.sidebar.header("Sobre o Projeto")
st.sidebar.markdown("""
**Objetivo:** Detectar e classificar assaltos em v√≠deos de c√¢meras de seguran√ßa usando:
- Estimativa de poses humanas (YOLOv8)
- Redes Neurais Recorrentes (RNNs)

**Caracter√≠sticas detect√°veis em assaltos:**
- Movimentos bruscos
- Padr√µes de intera√ß√£o entre pessoas
- Posturas indicativas de amea√ßa
- Comportamentos n√£o naturais
""")

# Footer
st.markdown("---")
st.markdown("Desenvolvido para detec√ß√£o autom√°tica de assaltos em c√¢meras de seguran√ßa")